{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Clustering and Classification of Headlines\n",
    "\n",
    "## Due Wednesday, October 26 @ 11:59pm EDT\n",
    "\n",
    "In the directory where you found this notebook, you also will find these files:\n",
    "<ol>\n",
    "<li><code>train_clickbait.txt</code>: 15000 clickbait headlines, one per line</li>\n",
    "<li><code>train_non_clickbait.txt</code>: 15000 news headlines, one per line</li>\n",
    "<li><code>test.txt</code>: 1000 clickbait headlines and 1000 news headlines, one per line</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "Add, commit, and push this notebook by the deadline with:\n",
    "<ol>\n",
    "    <li> code for all required components</li>\n",
    "    <li> answers to the boldfaced Q questions in the notebook</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem set, you will be working with a corpus of headlines, some of which are from typical clickbait sources and others which are from standard news sources. You can learn more about the corpus and some experiments that have been run with the corpus in the article that accompanied its release:\n",
    "\n",
    "Chakraborty, A., Paranjape, B., Kakarla, S., and Ganguly, N. 2016. Stop Clickbait: Detecting and preventing clickbaits in online news media. In <i>Proceedings of the IEEE International Conference on Advances in Social Networks Analysis and Mining (ASONAM</i>), pp. 9-16.\n",
    "\n",
    "First we will import some libraries. Then we read in the news (non-clickbait) headlines, remove stop words and digits, downcase, and save both the normalized lists of tokens and the original strings to two lists. You can just execute the entire cell below without modifying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import re\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "## Here I am just customizing the nltk English stop list\n",
    "stoplist = stopwords.words('english')\n",
    "stoplist.extend([\"ever\", \"one\", \"do\",\"does\",\"make\", \"go\", \"us\", \"to\", \"get\", \"about\", \"may\", \"s\", \".\", \",\", \"!\", \"i\", \"I\", '\\\"', \"?\", \";\", \"--\", \"--\", \"would\", \"could\", \"”\", \"Mr.\", \"Miss\", \"Mrs.\", \"don’t\", \"said\", \"can't\", \"didn't\", \"aren't\", \"I'm\", \"you're\", \"they're\", \"'s\"])\n",
    "stoplist.remove(\"which\")\n",
    "\n",
    "\n",
    "## Here I am reading in the news (non-clickbait headlines)\n",
    "newsheadlines = []     # this will store the original headline strings\n",
    "newsheadlinetoks = []  # this will store the lists of tokens in those headlines\n",
    "\n",
    "f = open(\"train_non_clickbait.txt\")\n",
    "for line in f:\n",
    "    line = line.rstrip()\n",
    "    newsheadlines.append(line)    \n",
    "    line = re.sub(r\"(^| )[0-9]+($| )\", r\" \", line)  # remove digits\n",
    "    addme = [t.lower() for t in line.split() if t.lower() not in stoplist]\n",
    "    newsheadlinetoks.append(addme)\n",
    "f.close()\n",
    "\n",
    "## Now, just printing out an example line from the original headline strings\n",
    "print(newsheadlines[50])\n",
    "\n",
    "## And printing out the normalized list of tokens for that string\n",
    "print(newsheadlinetoks[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identifying topics in news headlines\n",
    "\n",
    "In class, we learned a little bit about topic modeling with LDA. Here, we are going to try to identify topics in a different way using word2vec word embeddings. So first we need to load the word embedding model from Lab 5. <b>Move that model to this directory or change the path in the command below to your lab5 diretory so that you can execute the command below and load the model.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmodel = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300-SLIM.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will read in the normalized tokens for each headline, look up their vectors in the word2vec model, and sum them all up into a single vector per headline. As we discussed in class, word embeddings are especially cool because you can add them up and capture the semantics of an entire text.\n",
    "\n",
    "You can just execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsvectors = []   # this list will contain one 300-dimensional vector per headline\n",
    "\n",
    "for h in newsheadlinetoks:\n",
    "    totvec = np.zeros(300)\n",
    "    for w in h:\n",
    "        if w.lower() in bigmodel:\n",
    "            totvec = totvec + bigmodel[w.lower()]\n",
    "    newsvectors.append(totvec)\n",
    "\n",
    "print(len(newsvectors))\n",
    "print(len(newsheadlines))\n",
    "print(len(newsvectors[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, we discussed a clustering method called <b>k-means clustering</b>. This technique takes vectors and tries to group them together based on how far apart those vectors are from each other. As with LDA, you need to say how many clusters you want in advance. We're going to say 50.\n",
    "\n",
    "Again, you can just execute the code below. <b>Note: It will take a few minutes! Wait until the asterisk in the square brackets is replace by a number to continue</b>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmnews = KMeans(n_clusters=50, random_state=0)\n",
    "newsclusters = kmnews.fit_predict(newsvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what the clusters look like. The k-means fit_predict() function in scikit returns a list containing a single integer for every input vector corresponding to the cluster ID that vector was assigned to. We can simply iterate through that list of cluster assignments, and we can print out all the headlines that belong to one of the clusters. \n",
    "\n",
    "Execute the code below to see all the headlines in cluster 35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(newsclusters)):\n",
    "    if newsclusters[i] == 35:\n",
    "        print(newsheadlines[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: There are 50 clusters. Examine many different clusters of headlines by changing 35 in the code above to another integer between 1 and 50. Find <u>5</u> clusters of headlines that you think are good clusters. Report on these 5 clusters as follows: Make a table with three columns: (1) cluster ID; (2) your description of that cluster's topic; (3) 3  sentences from the cluster exemplifying that topic. \n",
    "\n",
    "### Some clusters might not immediately make a lot of sense, and there will probably be some headlines that don't perfectly fit with the rest in a particular cluster. If you can't figure out the topic of a cluster, just try a different one. It should be very easy to get 5 that are easy to categorize.\n",
    "\n",
    "### You can learn how to make a table in markdown [here](https://www.markdownguide.org/extended-syntax/) or just by doing a web search for \"table markdown\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer to Q1 goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identifying topics in clickbait headlines\n",
    "\n",
    "Repeat the above procedure on the file of clickbait headlines, which is called <code>train_clickbait.txt</code>. \n",
    "\n",
    "Any variables that have \"news\" in their names in the above code should have \"click\" in their name in your code. I've given you the variable names and descriptions in the cells below where you should put your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## READ IN THE DATA\n",
    "clickheadlines = []     # this will store the original clickbait headline strings\n",
    "clickheadlinetoks = []  # this will store the lists of tokens in those clickbait headlines\n",
    "\n",
    "\n",
    "## CREATE YOUR VECTORS\n",
    "clickvectors = []       # this list will contain one 300-dimensional vector per clickbait headline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN KMEANS TO CLUSTER YOUR DATA\n",
    "\n",
    "# Once you've built clickvectors in the cell above,\n",
    "# you can run this below to cluster the data.\n",
    "\n",
    "kmclick = KMeans(n_clusters=50, random_state=0)  \n",
    "clickclusters = kmclick.fit_predict(clickvectors)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRINT OUT HEADLINES IN A PARTICULAR CLUSTER\n",
    "\n",
    "# Once you have clustered your clickbait data\n",
    "# you can run this code to print out the headlines\n",
    "# in a particular cluster.\n",
    "\n",
    "for i in range(len(clickclusters)):\n",
    "    if clickclusters[i] == 35:\n",
    "        print(clickheadlines[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: There are 50 clusters of clickbait headlines. Examine many different clusters of headlines by changing 35 in the code above to another integer between 1 and 50. Find <u>5</u> clusters of headlines that you think are good clusters. Report on these 5 clusters as follows: Make a table with three columns: (1) cluster ID; (2) your description of that cluster's topic; (3) 3  sentences from the cluster exemplifying that topic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer to Q2 goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: How do these clusters compare with the clusters we saw using LDA in in class last Wednesday?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer to Q3 goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: How would you design an experiment to evaluate the quality of these clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer to Q4 goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Nearest neighbor classification\n",
    "\n",
    "Examine the file <code>test.txt</code>. The very first character of each line is either 1 or 0. A 1 indicates that the headline is clickbait. A 0 indicates that the headline is a news headline. The 1 or 0 is followed by a tab and then by the text of the headline itself.\n",
    "\n",
    "You are going to read in the data from <code>test.txt</code> and try to classify each headline as either clickbait or news. The first method you will use is the k-nearest neighborbors algorithm discussed a few weeks ago in class. To simplify things, we'll set k to be equal to 1.\n",
    "\n",
    "Here's how the algorithm works:\n",
    "<ol>\n",
    "    <li> Take an incoming headline, and sum the word embedding vectors of its component words, downcasing and removing stopwords, <u>exactly</u> as you did above with the training data. (Reuse that code!)</li>\n",
    "    <li> Compare that vector to every vector in <code>clickvectors</code> using <code>scipy.spatial.distance.cdist</code>. (See the code for details.)</li>\n",
    "    <li> Compare your vector to every vector in <code>newsvectors</code> with <code>scipy.spatial.distance.cdist</code></li>\n",
    "    <li> Find the mins in each row, as described in the code using <code>min</code></li>\n",
    "    <li> For each headline, if the news min is smaller than the clickbait min then classify that headline as 0 (news). Otherwise classify that headline as 1 (clickbait).</li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "Given this information, you must also write code to keep track of and report the performance of your k-nearest neighbors classifier. Report percent correct, precision, and recall.\n",
    "\n",
    "Note: <code>scipy.spatial.distance.cdist</code> takes the following arguments:\n",
    "* the 2D array containing your test vectors <code>testvectors</code>\n",
    "* the 2D array containing either the clickbait vectors or the news vectors\n",
    "\n",
    "And it returns a 2D array containing, for each row, the distance from that test vector to each of the clickbait or news vectors.\n",
    "\n",
    "<b>Do not use the built-in KNN function in scikit-learn.</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from sklearn import metrics\n",
    "\n",
    "## WRITE YOUR K-NEAREST NEIGHBORS CODE HERE\n",
    "## COMMENT YOUR CODE CLEARLY\n",
    "\n",
    "testtargets = []  # where to store whether a test headline is 0 or 1\n",
    "testvectors = []  # where to store the vector for each headline\n",
    "\n",
    "# while you read in test.txt...\n",
    "# ...keep track of whether each headline is 1 (clickbait) or 0 (news) in the list testtargets[]\n",
    "# ...AND get the summed word embedding vector for each headline and append it to list testvectors[]\n",
    "\n",
    "\n",
    "\n",
    "## SANITY CHECKING\n",
    "# len(testvectors) should equal 2000 and should be a list of lists\n",
    "# len(testvectors[100]) should equal 300\n",
    "# len(testtargets) should equal 2000 and should be a list of 1s and 0s\n",
    "\n",
    "\n",
    "## GET THE DISTANCES\n",
    "# get the distance between the each test vector and each of the clickbait vectors\n",
    "# use scipy.spatial.distance.cdist(testvectors, clickvectors)\n",
    "# save the output of cdist to a 2D array called clickdistances\n",
    "# each row will correspond to one test vector\n",
    "# each value in the row will correspond to the distance between that vector\n",
    "# and one of the new vectors\n",
    "\n",
    "\n",
    "# get the distance between the each test vector and each of the news vectors\n",
    "# use scipy.spatial.distance.cdist(testvectors, newsvectors)\n",
    "# save the output of cdist to a 2D array called newsdistances\n",
    "# each row will correspond to one test vector\n",
    "# each value in the row will correspond to the distance between that vector\n",
    "# and one of the new vectors\n",
    "\n",
    "\n",
    "## GET THE MIN DISTANCES\n",
    "# get the min of of each row in clickdistances using clickdistances.min(axis=1)\n",
    "# save out to a list or vector called clickmins\n",
    "\n",
    "# get the min of of each row in newsdistances using newsdistances.min(axis=1)\n",
    "# save out to a list or vector called newsmins\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## GET YOUR PREDICTIONS\n",
    "predictedknn = []  # where to store your KNN predictions \n",
    "\n",
    "# loop through the mins in newsmins and clickmins\n",
    "# if the news min is smaller than the click min, append 0 to predictedknn\n",
    "# otherwise append 1 to knnpredicted\n",
    "\n",
    "\n",
    "## EVALUATE YOUR PREDICTIONS\n",
    "# print the classification report\n",
    "print(metrics.classification_report(testtargets, predictedknn))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: What would be the random baseline for this dataset? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer to Q5 goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6: What percent of the headlines did you correctly classify with your KNN implementation? What were your precision and recall? How does this compare to the random baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer to Q6 goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification with word embedding features\n",
    "\n",
    "Now we are going to use some of the classifiers in scikit learn. First, we're going to create a classifier that will take, as features, the summed word embedding vector for a headline. \n",
    "\n",
    "We already have all our input for the classifier. We just need to put it in the right format by executing the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alltargets will just be a list of 1s and 0s indicating\n",
    "# which class each headline belongs to (clickbait or news)\n",
    "alltargets = list(np.ones(len(clickvectors)))\n",
    "alltargets.extend(np.zeros(len(newsvectors)))\n",
    "alltargets = np.array(alltargets)\n",
    "\n",
    "# allvectors is just the full set of word embedding vectors\n",
    "# for both clickbait and news headlines\n",
    "allvectors = clickvectors + newsvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below you've seen before in previous labs. All we are doing is training a model on the summed word embedding vector for each headline. First we trying our old friend naive Bayes, and then we try a linear SVM and then logistic regression. I've written the code for naive Bayes. You write the code for the other two classifiers. (Refer to previous labs to remember how to do this. It's easy!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "\n",
    "# NAIVE BAYES\n",
    "\n",
    "# Initialize the NB model\n",
    "model = GaussianNB()\n",
    "\n",
    "# Fit model to the training data\n",
    "model.fit(allvectors, alltargets)\n",
    "\n",
    "# Apply model to test set using predict()\n",
    "expected = testtargets\n",
    "predicted = model.predict(testvectors)\n",
    "\n",
    "# Print a classification report\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "\n",
    "\n",
    "### SVM\n",
    "\n",
    "# Initialize SVM model\n",
    "\n",
    "\n",
    "# Fit it to the the training data\n",
    "\n",
    "\n",
    "# Apply model to test set using predict()\n",
    "\n",
    "\n",
    "# Print a classification report\n",
    "\n",
    "\n",
    "### Logistic regression\n",
    "\n",
    "# Initialize the model\n",
    "\n",
    "\n",
    "# Fit it to the the training data\n",
    "\n",
    "\n",
    "# Apply model to test set using predict()\n",
    "\n",
    "\n",
    "# Print a classification report\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: Which performs best? How do they both compare to your k-nearest neighbors classifier? Why do you think this might be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer to Q7 goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classification with cluster features\n",
    "\n",
    "Finally, we will explore whether the distances to each of the clusters can be used as features for classification of clickbait vs. news. The code in the cell below will build the necessary input for you. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testclickdistances = kmclick.transform(testvectors)\n",
    "testnewsdistances = kmnews.transform(testvectors)\n",
    "\n",
    "clickdistances = kmclick.transform(allvectors)\n",
    "newsdistances = kmnews.transform(allvectors)\n",
    "\n",
    "# this vector tells you for each training headline, how far\n",
    "# away is it from each of the 100 clusters (50 news and 50 clickbait)\n",
    "allclusterdistances = np.column_stack( [clickdistances,newsdistances])\n",
    "\n",
    "# this vector tells you for each test headline, how far\n",
    "# away is it from each of the 100 clusters (50 news and 50 clickbait)\n",
    "testclusterdistances = np.column_stack( [testclickdistances,testnewsdistances])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in the cell below, train and test a naive Bayes classifier, a linear SVM, and logistic regression classifier using <code>allclusterdistances</code> intead of <code>allvectors</code> and <code>testclusterdistances</code> instead of <code>testvectors</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train (fit) your NB with allclusterdistances\n",
    "\n",
    "# Test your NB with testclusterdistances\n",
    "\n",
    "# Print classification report\n",
    "\n",
    "\n",
    "# Train (fit) your SVM with allclusterdistances\n",
    "\n",
    "# Test your SVM with testclusterdistances\n",
    "\n",
    "# Print classification report\n",
    "\n",
    "\n",
    "# Train (fit) your Logistic Regression with allclusterdistances\n",
    "\n",
    "# Test your LR with testclusterdistances\n",
    "\n",
    "# Print classification report\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8: Which performs best? How do they both compare to the models trained just on word embedding vectors, in part 5. Why do you think this might be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer to Q8 goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9: Create a table with all of the results from parts 4, 5, and 6. Discuss what you believe would be the best approach of these three for identifying clickbait. Then describe another experiment you would like to run to improve clickbait classification (e.g., using different features, using a different classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer to Q9 goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verifying and submitting your work\n",
    "Go up to the Kernel menu and select Restart and Run All. This will run all of the code you've written. Make sure there are no errors.\n",
    "\n",
    "**Add, commit, and push to your repo his notebook with both your code and your answers to the questions.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
